{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plotly Choropleth Maps - By Countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb3b9a740bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Setup dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "path = \"static/data/choropleth_locations3_all_sentiment.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all countries and codes\n",
    "\n",
    "df_countries_year = df.drop_duplicates(subset=['country'])\n",
    "df_countries_year = df_countries_year[['country', 'country_ISO_code']]\n",
    "df_countries_year = df_countries_year.sort_values('country').reset_index(drop=True)\n",
    "df_countries_year['article_score'] = \" \"\n",
    "df_countries_year['year'] = \" \"\n",
    "df_countries_year = df_countries_year[['country', 'country_ISO_code', 'article_score', 'year']]\n",
    "df_countries_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean scores by country by year\n",
    "\n",
    "df_coun_year = df[['country', 'country_ISO_code', 'latitude', 'longitude', 'article_score', 'year']]\n",
    "group_by_year = df_coun_year.groupby(['year', 'country', 'country_ISO_code'], as_index=False)['article_score'].mean()\n",
    "df_coun_year = pd.DataFrame({'country':group_by_year.country, 'country_ISO_code':group_by_year.country_ISO_code, 'article_score':group_by_year.article_score, 'year':group_by_year.year})\n",
    "\n",
    "for i in range(len(df_coun_year)):\n",
    "    df_coun_year['article_score'][i] = \"{:.3f}\".format(df_coun_year['article_score'][i])\n",
    "df_coun_year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create country dataframes for each year and combine to get a single master dataset\n",
    "\n",
    "# Extract and concat data for 2015\n",
    "df_country_2015 = df_countries_year\n",
    "for i in range(len(df_country_2015)):\n",
    "    df_country_2015['year'][i]  = 2015\n",
    "    for j in range(len(df_coun_year)):\n",
    "        if df_coun_year['year'][j] == 2015:\n",
    "            if df_country_2015['country'][i] == df_coun_year['country'][j]:     \n",
    "                df_country_2015['article_score'][i] = df_coun_year['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_2015['article_score'][i] == \" \":\n",
    "        df_country_2015['article_score'][i] = np.nan\n",
    "\n",
    "df_country_year = pd.DataFrame()\n",
    "df_country_year = pd.concat([df_country_year, df_country_2015], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for 2016\n",
    "df_country_2016 = df_countries_year\n",
    "for i in range(len(df_country_2016)):\n",
    "    df_country_2016['year'][i]  = 2016\n",
    "    for j in range(len(df_coun_year)):\n",
    "        if df_coun_year['year'][j] == 2016:\n",
    "            if df_country_2016['country'][i] == df_coun_year['country'][j]:     \n",
    "                df_country_2016['article_score'][i] = df_coun_year['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_2016['article_score'][i] == \" \":\n",
    "        df_country_2016['article_score'][i] = np.nan\n",
    "\n",
    "df_country_year = pd.concat([df_country_year, df_country_2016], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for 2017\n",
    "df_country_2017 = df_countries_year\n",
    "for i in range(len(df_country_2017)):\n",
    "    df_country_2017['year'][i]  = 2017\n",
    "    for j in range(len(df_coun_year)):\n",
    "        if df_coun_year['year'][j] == 2017:\n",
    "            if df_country_2017['country'][i] == df_coun_year['country'][j]:     \n",
    "                df_country_2017['article_score'][i] = df_coun_year['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_2017['article_score'][i] == \" \":\n",
    "        df_country_2017['article_score'][i] = np.nan\n",
    "\n",
    "df_country_year = pd.concat([df_country_year, df_country_2017], ignore_index=True).dropna()\n",
    "df_country_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries - final data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_2015 = df_country_year.loc[df_country_year['year'] == 2015]\n",
    "country_2016 = df_country_year.loc[df_country_year['year'] == 2016]\n",
    "country_2017 = df_country_year.loc[df_country_year['year'] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create choropleth map of sentiment scores by country (year 2015 only)\n",
    "\n",
    "fig_global_2015 = go.Figure(data=go.Choropleth(\n",
    "    locations = country_2015['country_ISO_code'],\n",
    "    z = country_2015['article_score'],\n",
    "    text = country_2015['country'],\n",
    "    colorscale = 'RdBu',\n",
    "    marker_line_color='darkgray',\n",
    "    marker_line_width=1,\n",
    "    colorbar_title = 'Sentiment Score',\n",
    "))\n",
    "\n",
    "fig_global_2015.update_layout(\n",
    "    title_text='Global News Sentiment by Country (2015)',\n",
    "    width=1150,\n",
    "    height=1150,\n",
    "    margin=dict(l=20, r=20, b=300, t=100, pad=10),\n",
    "    font=dict(size=20),\n",
    "    geo=dict(\n",
    "        showframe=False,\n",
    "        showcoastlines=False,\n",
    "        projection_type='equirectangular'\n",
    "    ),\n",
    "    annotations = [dict(\n",
    "        x=0.55,\n",
    "        y=0.35,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        text='Source: <a href=\"https://www.nytimes.com/\"> The New York Times</a>',\n",
    "        showarrow = False\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig_global_2015.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create choropleth map of sentiment scores by country (year 2016 only)\n",
    "\n",
    "fig_global_2016 = go.Figure(data=go.Choropleth(\n",
    "    locations = country_2016['country_ISO_code'],\n",
    "    z = country_2016['article_score'],\n",
    "    text = country_2016['country'],\n",
    "    colorscale = 'RdBu',\n",
    "    marker_line_color='darkgray',\n",
    "    marker_line_width=1,\n",
    "    colorbar_title = 'Sentiment Score',\n",
    "))\n",
    "\n",
    "fig_global_2016.update_layout(\n",
    "    title_text='Global News Sentiment by Country (2016)',\n",
    "    width=1150,\n",
    "    height=1150,\n",
    "    margin=dict(l=20, r=20, b=300, t=100, pad=10),\n",
    "    font=dict(size=20),\n",
    "    geo=dict(\n",
    "        showframe=False,\n",
    "        showcoastlines=False,\n",
    "        projection_type='equirectangular'\n",
    "    ),\n",
    "    annotations = [dict(\n",
    "        x=0.55,\n",
    "        y=0.35,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        text='Source: <a href=\"https://www.nytimes.com/\"> The New York Times</a>',\n",
    "        showarrow = False\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig_global_2016.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create choropleth map of sentiment scores by country (year 2017 only)\n",
    "\n",
    "fig_global_2017 = go.Figure(data=go.Choropleth(\n",
    "    locations = country_2017['country_ISO_code'],\n",
    "    z = country_2017['article_score'],\n",
    "    text = country_2017['country'],\n",
    "    colorscale = 'RdBu',\n",
    "    marker_line_color='darkgray',\n",
    "    marker_line_width=1,\n",
    "    colorbar_title = 'Sentiment Score',\n",
    "))\n",
    "\n",
    "fig_global_2017.update_layout(\n",
    "    title_text='Global News Sentiment by Country (2017)',\n",
    "    width=1150,\n",
    "    height=1150,\n",
    "    margin=dict(l=20, r=20, b=300, t=100, pad=10),\n",
    "    font=dict(size=20),\n",
    "    geo=dict(\n",
    "        showframe=False,\n",
    "        showcoastlines=False,\n",
    "        projection_type='equirectangular'\n",
    "    ),\n",
    "    annotations = [dict(\n",
    "        x=0.55,\n",
    "        y=0.35,\n",
    "        xref='paper',\n",
    "        yref='paper',\n",
    "        text='Source: <a href=\"https://www.nytimes.com/\"> The New York Times</a>',\n",
    "        showarrow = False\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig_global_2017.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries - Yearly Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation\n",
    "\n",
    "# fig = px.choropleth(df_country_year,              \n",
    "#             locations=\"country_ISO_code\",               \n",
    "#             color=\"article_score\",\n",
    "#             hover_name=\"country\",  \n",
    "#             animation_frame=\"year\",    \n",
    "#             color_continuous_scale=\"Viridis\",\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title_text='Global News Sentiment 2015-2017',\n",
    "#     width=1150,\n",
    "#     height=1150,\n",
    "#     margin=dict(l=20, r=20, b=300, t=100, pad=10),\n",
    "#     font=dict(size=20),\n",
    "#     geo=dict(\n",
    "#         showframe=False,\n",
    "#         showcoastlines=False,\n",
    "#         projection_type='equirectangular'\n",
    "#     ),\n",
    "#     annotations = [dict(\n",
    "#         x=0.55,\n",
    "#         y=0.35,\n",
    "#         xref='paper',\n",
    "#         yref='paper',\n",
    "#         text='Source: <a href=\"https://www.nytimes.com/\"> The New York Times</a>',\n",
    "#         showarrow = False\n",
    "#     )]\n",
    "# )\n",
    "\n",
    "px.choropleth(df_country_year,               \n",
    "              locations=\"country_ISO_code\",               \n",
    "              color=\"article_score\",\n",
    "              hover_name=\"country\",  \n",
    "              animation_frame=\"year\",    \n",
    "              color_continuous_scale='Plasma',  \n",
    "              height=600             \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries - final data by weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create a dataframe with all countries and codes\n",
    "\n",
    "df_countries_weekday = df.drop_duplicates(subset=['country'])\n",
    "df_countries_weekday = df_countries_weekday[['country', 'country_ISO_code']]\n",
    "df_countries_weekday = df_countries_weekday.sort_values('country').reset_index(drop=True)\n",
    "df_countries_weekday['article_score'] = \" \"\n",
    "df_countries_weekday['weekday'] = \" \"\n",
    "df_countries_weekday = df_countries_weekday[['country', 'country_ISO_code', 'article_score', 'weekday']]\n",
    "df_countries_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean scores by country by day of the week\n",
    "\n",
    "df_coun_weekday = df[['country', 'country_ISO_code', 'article_score', 'weekday']]\n",
    "group_by_weekday = df_coun_weekday.groupby(['country', 'country_ISO_code', 'weekday'], as_index=False)['article_score'].mean()\n",
    "df_coun_weekday = pd.DataFrame({'country':group_by_weekday.country, 'country_ISO_code':group_by_weekday.country_ISO_code, 'article_score':group_by_weekday.article_score, 'weekday':group_by_weekday.weekday})\n",
    "\n",
    "for i in range(len(df_coun_weekday)):\n",
    "    df_coun_weekday['article_score'][i] = \"{:.3f}\".format(df_coun_weekday['article_score'][i])\n",
    "df_coun_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create country dataframes for each weekday and combine to get a single master dataset\n",
    "\n",
    "# Extract and concat data for Monday\n",
    "df_country_mon = df_countries_weekday\n",
    "for i in range(len(df_country_mon)):\n",
    "    df_country_mon['weekday'][i]  = \"Monday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Monday\":\n",
    "            if df_country_mon['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_mon['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_mon['article_score'][i] == \" \":\n",
    "        df_country_mon['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.DataFrame()\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_mon], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Tuesday\n",
    "df_country_tue = df_countries_weekday\n",
    "for i in range(len(df_country_tue)):\n",
    "    df_country_tue['weekday'][i]  = \"Tuesday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Tuesday\":\n",
    "            if df_country_tue['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_tue['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_tue['article_score'][i] == \" \":\n",
    "        df_country_tue['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.DataFrame()\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_tue], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Wednesday\n",
    "df_country_wed = df_countries_weekday\n",
    "for i in range(len(df_country_wed)):\n",
    "    df_country_wed['weekday'][i]  = \"Wednesday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Wednesday\":\n",
    "            if df_country_wed['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_wed['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_wed['article_score'][i] == \" \":\n",
    "        df_country_wed['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_wed], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Thursday\n",
    "df_country_thur = df_countries_weekday\n",
    "for i in range(len(df_country_thur)):\n",
    "    df_country_thur['weekday'][i]  = \"Thursday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Thursday\":\n",
    "            if df_country_thur['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_thur['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_thur['article_score'][i] == \" \":\n",
    "        df_country_thur['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_thur], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Friday\n",
    "df_country_fri = df_countries_weekday\n",
    "for i in range(len(df_country_fri)):\n",
    "    df_country_fri['weekday'][i]  = \"Friday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Friday\":\n",
    "            if df_country_fri['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_fri['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_fri['article_score'][i] == \" \":\n",
    "        df_country_fri['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_fri], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Saturday\n",
    "df_country_sat = df_countries_weekday\n",
    "for i in range(len(df_country_sat)):\n",
    "    df_country_sat['weekday'][i]  = \"Saturday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Saturday\":\n",
    "            if df_country_sat['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_sat['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_sat['article_score'][i] == \" \":\n",
    "        df_country_sat['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_sat], ignore_index=True)\n",
    "\n",
    "\n",
    "# Extract and concat data for Sunday\n",
    "df_country_sun = df_countries_weekday\n",
    "for i in range(len(df_country_sun)):\n",
    "    df_country_sun['weekday'][i]  = \"Sunday\"\n",
    "    for j in range(len(df_coun_weekday)):\n",
    "        if df_coun_weekday['weekday'][j] == \"Sunday\":\n",
    "            if df_country_sun['country'][i] == df_coun_weekday['country'][j]:     \n",
    "                df_country_sun['article_score'][i] = df_coun_weekday['article_score'][j]\n",
    "            else:\n",
    "                continue\n",
    "        else:     \n",
    "            continue\n",
    "    if df_country_sun['article_score'][i] == \" \":\n",
    "        df_country_sun['article_score'][i] = np.nan\n",
    "\n",
    "df_country_weekday = pd.concat([df_country_weekday, df_country_sun], ignore_index=True)\n",
    "\n",
    "\n",
    "df_country_weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries - final data by weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_monday = df_countrye_weekday.loc[df_country_weekday['weekday'] == \"Monday\"]\n",
    "country_tuesday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Tuesday\"]\n",
    "country_wednesday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Wednesday\"]\n",
    "country_thursday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Thursday\"]\n",
    "country_friday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Friday\"]\n",
    "country_saturday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Saturday\"]\n",
    "country_sunday = df_country_weekday.loc[df_country_weekday['weekday'] == \"Sunday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries - Weekday Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.plotly as py\n",
    "\n",
    "# # min year in your dataset\n",
    "# year = 2015\n",
    "# # years = ['2015', '2016', '2017']\n",
    "# # your color-scale\n",
    "# colorscale = 'RdBu'\n",
    "# # scl = [[0.0, '#ffffff'],[0.2, '#b4a8ce'],[0.4, '#8573a9'],\n",
    "# #        [0.6, '#7159a3'],[0.8, '#5732a1'],[1.0, '#2c0579']] # purples\n",
    "\n",
    "\n",
    "# data_slider = []\n",
    "# for year in df['years'].unique():\n",
    "#     df_segmented =  df[(df['years']== year)]\n",
    "\n",
    "#     for col in df_segmented.columns:\n",
    "#         df_segmented[col] = df_segmented[col].astype(str)\n",
    "\n",
    "#     data_each_yr = dict(\n",
    "#                         type='choropleth',\n",
    "#                         locations = df_segmented['state'],\n",
    "#                         z=df_segmented['sightings'].astype(float),\n",
    "#                         locationmode='USA-states',\n",
    "#                         colorscale = scl,\n",
    "#                         colorbar= {'title':'# Sightings'})\n",
    "\n",
    "#     data_slider.append(data_each_yr)\n",
    "\n",
    "# steps = []\n",
    "# for i in range(len(data_slider)):\n",
    "#     step = dict(method='restyle',\n",
    "#                 args=['visible', [False] * len(data_slider)],\n",
    "#                 label='Year {}'.format(i + 1998))\n",
    "#     step['args'][1][i] = True\n",
    "#     steps.append(step)\n",
    "\n",
    "# sliders = [dict(active=0, pad={\"t\": 1}, steps=steps)]\n",
    "\n",
    "# layout = dict(title ='UFO Sightings by State Since 1998', geo=dict(scope='usa',\n",
    "#                        projection={'type': 'albers usa'}),\n",
    "#               sliders=sliders)\n",
    "\n",
    "# fig = dict(data=data_slider, layout=layout)\n",
    "# periscope.plotly(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GIF for the yearly global plots:\n",
    "years = ['2015', '2016', '2017']\n",
    "\n",
    "# for year in years:\n",
    "#     fig(year)\n",
    "fig(2015) = './news_app/static/img/fig_global_2015.png'\n",
    "fig(2016) = './news_app/static/img/fig_global_2016.png'\n",
    "fig(2017) = './news_app/static/img/fig_global_2017.png'\n",
    "\n",
    "\n",
    "images = [fig(2015), fig(2016), fig(2017)]\n",
    "# looping over the images and saving them into a list\n",
    "for i in range(images):\n",
    "  images.append(images)\n",
    "\n",
    "# creating the GIF\n",
    "images[0].save('./news_app/static/img/fig_global_2015-16-17.gif',\n",
    "               save_all=True, append_images=images[1:], optimize=True, duration=800, loop=0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
